{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3919937,"sourceType":"datasetVersion","datasetId":2327240}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nimport json\n\nBASE_DIR = Path(\"/kaggle/input/textocr-text-extraction-from-images-dataset\")\nIMG_DIR = BASE_DIR / \"train_val_images/train_images\"\n\nannot = pd.read_parquet(BASE_DIR / \"annot.parquet\")\nimgs = pd.read_parquet(BASE_DIR / \"img.parquet\").rename(columns={\"id\": \"image_id\"})\n\n# Group annotations by image_id\ngrouped = (\n    annot.groupby(\"image_id\")\n    .agg(bbox_list=(\"bbox\", list), text_list=(\"utf8_string\", list))\n    .reset_index()\n)\n\ndf = imgs.merge(grouped, on=\"image_id\", how=\"inner\")\n\ndf[\"image_path\"] = df[\"file_name\"].apply(\n    lambda fn: str(IMG_DIR / Path(fn).name)\n)\n\n# ---------------------------\n# Normalize bounding boxes to 0–1000\n# ----------------------------\ndef normalize_bbox(bbox, width, height):\n    x, y, w, h = bbox\n    x1, y1 = x, y\n    x2, y2 = x + w, y + h\n    return [\n        max(0, min(1000, int((x1 / width) * 1000))),\n        max(0, min(1000, int((y1 / height) * 1000))),\n        max(0, min(1000, int((x2 / width) * 1000))),\n        max(0, min(1000, int((y2 / height) * 1000))),\n    ]\n\ndf[\"annotations\"] = df.apply(\n    lambda row: [\n        {\n            \"bbox_2d\": normalize_bbox(bbox, row[\"width\"], row[\"height\"]),\n            \"text_content\": str(text)\n        }\n        for bbox, text in zip(row[\"bbox_list\"], row[\"text_list\"])\n    ],\n    axis=1\n)\n\n# ----------------------------\n# Build messages with JSON block\n# ----------------------------\ndef build_message(row):\n    annotations_json = json.dumps(row[\"annotations\"], ensure_ascii=False)\n    annotations_json_block = f\"```json\\n{annotations_json}\\n```\"\n\n    return {\n        \"image\": row[\"image_path\"],\n        \"conversations\": [\n            {\n                \"from\": \"human\",\n                \"value\": (\n                    \"<image> Spot all text in the image at line-level. \"\n                    \"Output the result in valid JSON format like below:\\n\"\n                    \"```json\\n[{'bbox_2d': [x1, y1, x2, y2], 'text_content': 'text'}, ...]\\n```\"\n                )\n            },\n            {\n                \"from\": \"assistant\",\n                \"value\": annotations_json_block\n            }\n        ]\n    }\n\ndf[\"message\"] = df.apply(build_message, axis=1)\n\n# ----------------------------\n# print first decoded message\n# ----------------------------\nimport pprint\npprint.pprint(df[\"message\"].iloc[0])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:20.171850Z","iopub.execute_input":"2026-02-14T18:04:20.172073Z","iopub.status.idle":"2026-02-14T18:04:34.404758Z","shell.execute_reply.started":"2026-02-14T18:04:20.172041Z","shell.execute_reply":"2026-02-14T18:04:34.404093Z"}},"outputs":[{"name":"stdout","text":"{'conversations': [{'from': 'human',\n                    'value': '<image> Spot all text in the image at '\n                             'line-level. Output the result in valid JSON '\n                             'format like below:\\n'\n                             '```json\\n'\n                             \"[{'bbox_2d': [x1, y1, x2, y2], 'text_content': \"\n                             \"'text'}, ...]\\n\"\n                             '```'},\n                   {'from': 'assistant',\n                    'value': '```json\\n'\n                             '[{\"bbox_2d\": [625, 3, 861, 36], \"text_content\": '\n                             '\"Performance\"}, {\"bbox_2d\": [636, 63, 745, 100], '\n                             '\"text_content\": \"Sport\"}, {\"bbox_2d\": [746, 62, '\n                             '861, 93], \"text_content\": \"Watch\"}, {\"bbox_2d\": '\n                             '[687, 138, 862, 180], \"text_content\": '\n                             '\"...period.\"}, {\"bbox_2d\": [465, 160, 537, 197], '\n                             '\"text_content\": \".\"}, {\"bbox_2d\": [542, 199, '\n                             '568, 223], \"text_content\": \"400\"}, {\"bbox_2d\": '\n                             '[564, 229, 585, 253], \"text_content\": \"300\"}, '\n                             '{\"bbox_2d\": [602, 257, 633, 298], '\n                             '\"text_content\": \"15\"}, {\"bbox_2d\": [412, 172, '\n                             '477, 210], \"text_content\": \"12\"}, {\"bbox_2d\": '\n                             '[433, 159, 457, 175], \"text_content\": \"60\"}, '\n                             '{\"bbox_2d\": [373, 168, 395, 187], '\n                             '\"text_content\": \"65\"}, {\"bbox_2d\": [327, 194, '\n                             '357, 221], \"text_content\": \"170\"}, {\"bbox_2d\": '\n                             '[307, 229, 333, 254], \"text_content\": \".\"}, '\n                             '{\"bbox_2d\": [305, 265, 323, 287], '\n                             '\"text_content\": \".\"}, {\"bbox_2d\": [310, 295, '\n                             '331, 317], \"text_content\": \".\"}, {\"bbox_2d\": '\n                             '[321, 318, 348, 342], \"text_content\": \".\"}, '\n                             '{\"bbox_2d\": [359, 355, 388, 376], '\n                             '\"text_content\": \"100\"}, {\"bbox_2d\": [749, 553, '\n                             '889, 600], \"text_content\": \"GTOR®\"}, {\"bbox_2d\": '\n                             '[256, 250, 296, 305], \"text_content\": \"45\"}, '\n                             '{\"bbox_2d\": [414, 229, 482, 247], '\n                             '\"text_content\": \".\"}, {\"bbox_2d\": [431, 241, '\n                             '465, 254], \"text_content\": \".\"}, {\"bbox_2d\": '\n                             '[402, 259, 419, 274], \"text_content\": \"10\"}, '\n                             '{\"bbox_2d\": [402, 281, 419, 296], '\n                             '\"text_content\": \"20\"}, {\"bbox_2d\": [381, 293, '\n                             '395, 305], \"text_content\": \"30\"}, {\"bbox_2d\": '\n                             '[357, 281, 374, 297], \"text_content\": \"40\"}, '\n                             '{\"bbox_2d\": [354, 259, 372, 276], '\n                             '\"text_content\": \"50\"}, {\"bbox_2d\": [378, 249, '\n                             '392, 261], \"text_content\": \".\"}]\\n'\n                             '```'}],\n 'image': '/kaggle/input/textocr-text-extraction-from-images-dataset/train_val_images/train_images/a4ea732cd3d5948a.jpg'}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import re\nfrom pathlib import Path\nfrom typing import Dict, Any, List\nimport torch\n\nIGNORE_INDEX = -100  \n\n# ----------------------------\n# Helper: make absolute paths\n# ----------------------------\ndef _make_abs_paths(base_path: Path, file_path: str) -> str:\n    return str(base_path / file_path)\n\n\n# ----------------------------\n# Build chat messages from item\n# ----------------------------\ndef _build_messages(item: Dict[str, Any], base_path: Path) -> List[Dict[str, Any]]:\n    # Handle images/videos\n    images = item.get(\"image\") or []\n    if isinstance(images, str):\n        images = [images]\n\n    videos = item.get(\"video\") or []\n    if isinstance(videos, str):\n        videos = [videos]\n\n    image_pool = [{\"type\": \"image\", \"image\": _make_abs_paths(base_path, img)} for img in images]\n    video_pool = [{\"type\": \"video\", \"video\": _make_abs_paths(base_path, vid)} for vid in videos]\n\n    messages = []\n\n    for turn in item[\"conversations\"]:\n        role = \"user\" if turn[\"from\"] == \"human\" else \"assistant\"\n        text: str = turn[\"value\"]\n\n        if role == \"user\":\n            content: List[Dict[str, Any]] = []\n            # Split on placeholders like <image> or <video>\n            text_parts = re.split(r\"(<image>|<video>)\", text)\n\n            for seg in text_parts:\n                seg = seg.strip()\n                if seg == \"<image>\":\n                    if not image_pool:\n                        raise ValueError(\"Too many <image> placeholders\")\n                    content.append(image_pool.pop(0))\n                elif seg == \"<video>\":\n                    if not video_pool:\n                        raise ValueError(\"Too many <video> placeholders\")\n                    content.append(video_pool.pop(0))\n                elif seg:\n                    content.append({\"type\": \"text\", \"text\": seg})\n\n            messages.append({\"role\": role, \"content\": content})\n\n        else:  # assistant\n            messages.append({\n                \"role\": role,\n                \"content\": [{\"type\": \"text\", \"text\": text}]\n            })\n\n    if image_pool:\n        raise ValueError(f\"{len(image_pool)} unused image(s)\")\n    if video_pool:\n        raise ValueError(f\"{len(video_pool)} unused video(s)\")\n\n    return messages\n\n\n# ----------------------------\n# Preprocess for Qwen visual\n# ----------------------------\ndef preprocess_qwen_visual_batch(sources: List[Dict[str, Any]], processor) -> Dict[str, Any]:\n    all_messages = []\n    for source in sources:\n        base_path = Path(source.get(\"data_path\", \"\"))\n        messages = _build_messages(source, base_path)\n        all_messages.append(messages)\n\n    # Apply processor to batch of messages with padding\n    full_result = processor.apply_chat_template(\n        all_messages,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\",\n        padding=True,\n        pad_to_multiple_of=8\n    )\n\n    input_ids = full_result[\"input_ids\"]\n    if isinstance(input_ids, list):\n        input_ids = torch.tensor(input_ids)\n\n    # Prepare labels with IGNORE_INDEX\n    labels = torch.full_like(input_ids, IGNORE_INDEX)\n\n    # Qwen special token logic for masking assistant answers\n    for batch_idx in range(input_ids.size(0)):\n        input_ids_flat = input_ids[batch_idx].tolist()\n        L = len(input_ids_flat)\n        pos = 0\n        while pos < L:\n            if input_ids_flat[pos] == 77091:  # <|im_start|>assistant token\n                ans_start = pos + 2\n                ans_end = ans_start\n                while ans_end < L and input_ids_flat[ans_end] != 151645:  # <|im_end|> token\n                    ans_end += 1\n                if ans_end < L:\n                    labels[batch_idx, ans_start:ans_end + 2] = input_ids[batch_idx, ans_start:ans_end + 2]\n                    pos = ans_end\n            pos += 1\n\n    full_result[\"input_ids\"] = input_ids\n    full_result[\"labels\"] = labels\n    return full_result\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:34.405595Z","iopub.execute_input":"2026-02-14T18:04:34.405900Z","iopub.status.idle":"2026-02-14T18:04:35.890142Z","shell.execute_reply.started":"2026-02-14T18:04:34.405877Z","shell.execute_reply":"2026-02-14T18:04:35.889515Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Flash attention 2 will work for Ampere and above series \n# !pip install flash-attn --no-build-isolation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:35.891012Z","iopub.execute_input":"2026-02-14T18:04:35.891467Z","iopub.status.idle":"2026-02-14T18:04:35.894824Z","shell.execute_reply.started":"2026-02-14T18:04:35.891442Z","shell.execute_reply":"2026-02-14T18:04:35.894112Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install bitsandbytes peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:35.896453Z","iopub.execute_input":"2026-02-14T18:04:35.896672Z","iopub.status.idle":"2026-02-14T18:04:39.319162Z","shell.execute_reply.started":"2026-02-14T18:04:35.896652Z","shell.execute_reply":"2026-02-14T18:04:39.318463Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\nRequirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (26.0rc2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2026.1.4)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom transformers import AutoProcessor , Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nfrom pathlib import Path\n\nIGNORE_INDEX = -100  \nmodel_name = \"Qwen/Qwen3-VL-2B-Instruct\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,  \n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\ndevice_to_use = \"cuda:0\"\n\nmodel = Qwen3VLForConditionalGeneration.from_pretrained(\n    model_name,\n    device_map={\"\":device_to_use},\n    quantization_config=bnb_config,\n    dtype=torch.float16,      \n    trust_remote_code=True,\n    attn_implementation=\"sdpa\"\n)\n\n# Load processor for VL tasks\nprocessor = AutoProcessor.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:39.320562Z","iopub.execute_input":"2026-02-14T18:04:39.320896Z","iopub.status.idle":"2026-02-14T18:04:57.874303Z","shell.execute_reply.started":"2026-02-14T18:04:39.320855Z","shell.execute_reply":"2026-02-14T18:04:57.873670Z"}},"outputs":[{"name":"stderr","text":"2026-02-14 18:04:41.764835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771092281.786404    1159 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771092281.792798    1159 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771092281.810035    1159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771092281.810054    1159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771092281.810056    1159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771092281.810059    1159 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"rows = [\n    df[\"message\"].iloc[0],\n    df[\"message\"].iloc[1]  # another row\n]\n\n# Convert rows to proper dict format for preprocess\nsources = []\nfor row in rows:\n    images = row.get(\"image\")\n    if isinstance(images, str):\n        images = [images]\n    sources.append({\n        \"image\": images,\n        \"conversations\": row[\"conversations\"]\n    })\n\nresult = preprocess_qwen_visual_batch(sources, processor)\n\nprint(result[\"input_ids\"].shape)  \nprint(result[\"labels\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:57.875212Z","iopub.execute_input":"2026-02-14T18:04:57.875819Z","iopub.status.idle":"2026-02-14T18:04:57.989056Z","shell.execute_reply.started":"2026-02-14T18:04:57.875790Z","shell.execute_reply":"2026-02-14T18:04:57.988205Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 2328])\ntorch.Size([2, 2328])\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(result.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:57.990348Z","iopub.execute_input":"2026-02-14T18:04:57.990647Z","iopub.status.idle":"2026-02-14T18:04:57.997733Z","shell.execute_reply.started":"2026-02-14T18:04:57.990618Z","shell.execute_reply":"2026-02-14T18:04:57.997091Z"}},"outputs":[{"name":"stdout","text":"KeysView({'input_ids': tensor([[151644,    872,    198,  ..., 151643, 151643, 151643],\n        [151644,    872,    198,  ...,  73594, 151645,    198]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 1, 1, 1]]), 'pixel_values': tensor([[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n        ...,\n        [-0.0588, -0.0431, -0.0431,  ..., -0.4902, -0.4588, -0.4118],\n        [-0.0824, -0.0980, -0.0980,  ..., -0.4745, -0.4902, -0.4431],\n        [ 0.0431,  0.0431,  0.0118,  ..., -0.4588, -0.5451, -0.5922]]), 'image_grid_thw': tensor([[ 1, 64, 52],\n        [ 1, 42, 64]]), 'labels': tensor([[  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n        [  -100,   -100,   -100,  ...,  73594, 151645,    198]])})\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"Input IDs:\\n\", result[\"input_ids\"])\nprint(\"\\nDecoded Text:\\n\", processor.batch_decode(result[\"input_ids\"])[0])\nprint(\"\\nLabels:\\n\", result[\"labels\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:57.998523Z","iopub.execute_input":"2026-02-14T18:04:57.999182Z","iopub.status.idle":"2026-02-14T18:04:58.010238Z","shell.execute_reply.started":"2026-02-14T18:04:57.999140Z","shell.execute_reply":"2026-02-14T18:04:58.009589Z"}},"outputs":[{"name":"stdout","text":"Input IDs:\n tensor([[151644,    872,    198,  ..., 151643, 151643, 151643],\n        [151644,    872,    198,  ...,  73594, 151645,    198]])\n\nDecoded Text:\n <|im_start|>user\n<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>Spot all text in the image at line-level. Output the result in valid JSON format like below:\n```json\n[{'bbox_2d': [x1, y1, x2, y2], 'text_content': 'text'}, ...]\n```<|im_end|>\n<|im_start|>assistant\n```json\n[{\"bbox_2d\": [625, 3, 861, 36], \"text_content\": \"Performance\"}, {\"bbox_2d\": [636, 63, 745, 100], \"text_content\": \"Sport\"}, {\"bbox_2d\": [746, 62, 861, 93], \"text_content\": \"Watch\"}, {\"bbox_2d\": [687, 138, 862, 180], \"text_content\": \"...period.\"}, {\"bbox_2d\": [465, 160, 537, 197], \"text_content\": \".\"}, {\"bbox_2d\": [542, 199, 568, 223], \"text_content\": \"400\"}, {\"bbox_2d\": [564, 229, 585, 253], \"text_content\": \"300\"}, {\"bbox_2d\": [602, 257, 633, 298], \"text_content\": \"15\"}, {\"bbox_2d\": [412, 172, 477, 210], \"text_content\": \"12\"}, {\"bbox_2d\": [433, 159, 457, 175], \"text_content\": \"60\"}, {\"bbox_2d\": [373, 168, 395, 187], \"text_content\": \"65\"}, {\"bbox_2d\": [327, 194, 357, 221], \"text_content\": \"170\"}, {\"bbox_2d\": [307, 229, 333, 254], \"text_content\": \".\"}, {\"bbox_2d\": [305, 265, 323, 287], \"text_content\": \".\"}, {\"bbox_2d\": [310, 295, 331, 317], \"text_content\": \".\"}, {\"bbox_2d\": [321, 318, 348, 342], \"text_content\": \".\"}, {\"bbox_2d\": [359, 355, 388, 376], \"text_content\": \"100\"}, {\"bbox_2d\": [749, 553, 889, 600], \"text_content\": \"GTOR®\"}, {\"bbox_2d\": [256, 250, 296, 305], \"text_content\": \"45\"}, {\"bbox_2d\": [414, 229, 482, 247], \"text_content\": \".\"}, {\"bbox_2d\": [431, 241, 465, 254], \"text_content\": \".\"}, {\"bbox_2d\": [402, 259, 419, 274], \"text_content\": \"10\"}, {\"bbox_2d\": [402, 281, 419, 296], \"text_content\": \"20\"}, {\"bbox_2d\": [381, 293, 395, 305], \"text_content\": \"30\"}, {\"bbox_2d\": [357, 281, 374, 297], \"text_content\": \"40\"}, {\"bbox_2d\": [354, 259, 372, 276], \"text_content\": \"50\"}, {\"bbox_2d\": [378, 249, 392, 261], \"text_content\": \".\"}]\n```<|im_end|>\n<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n\nLabels:\n tensor([[  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n        [  -100,   -100,   -100,  ...,  73594, 151645,    198]])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Note: These parameters are subject to the VRAM of GPU , if you have a better GPU feel free to tune the Config params\n\nMax_Context_Length = 1024\nMax_pixels = 196*32*32 # increase it \nMin_pixels = 56*56\nFactor = 32\nMax_Long_Side = 8192","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:58.011111Z","iopub.execute_input":"2026-02-14T18:04:58.011356Z","iopub.status.idle":"2026-02-14T18:04:58.018410Z","shell.execute_reply.started":"2026-02-14T18:04:58.011324Z","shell.execute_reply":"2026-02-14T18:04:58.017724Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import math\n\ndef round_by_factor(number, factor):\n    return round(number / factor) * factor\n\ndef floor_by_factor(number, factor):\n    return math.floor(number / factor) * factor\n\ndef ceil_by_factor(number, factor):\n    return math.ceil(number / factor) * factor\n\ndef smart_resize(height: int, width: int, factor: int = Factor, \n                 min_pixels: int = Min_pixels, \n                 max_pixels: int = Max_pixels, \n                 max_long_side: int = Max_Long_Side) -> tuple[int, int]:\n    \n    \n    if max(height, width) > max_long_side:\n        beta = max(height, width) / max_long_side\n        height, width = int(height / beta), int(width / beta)\n\n    h_bar = round_by_factor(height, factor)\n    w_bar = round_by_factor(width, factor)\n    \n    if h_bar * w_bar > max_pixels:\n        # Calculate scaling beta based on the area ratio\n        beta = math.sqrt((height * width) / max_pixels)\n        h_bar = floor_by_factor(height / beta, factor)\n        w_bar = floor_by_factor(width / beta, factor)\n    elif h_bar * w_bar < min_pixels:\n        beta = math.sqrt(min_pixels / (height * width))\n        h_bar = ceil_by_factor(height * beta, factor)\n        w_bar = ceil_by_factor(width * beta, factor)\n    \n    return h_bar, w_bar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:58.019287Z","iopub.execute_input":"2026-02-14T18:04:58.019567Z","iopub.status.idle":"2026-02-14T18:04:58.029053Z","shell.execute_reply.started":"2026-02-14T18:04:58.019537Z","shell.execute_reply":"2026-02-14T18:04:58.028373Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom PIL import Image\nfrom pathlib import Path\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\nclass QwenVisualDataset(Dataset):\n    def __init__(self, dataframe):\n        self.data = dataframe.to_dict('records')\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data[idx]\n        item = row[\"message\"]\n        base_path = Path(item.get(\"data_path\", \"\"))\n        messages = _build_messages(item, base_path)\n        \n        for msg in messages:\n            if isinstance(msg[\"content\"], list):\n                for part in msg[\"content\"]:\n                    if part[\"type\"] == \"image\" and isinstance(part[\"image\"], str):\n                        img = Image.open(part[\"image\"]).convert(\"RGB\")\n                        \n                        # Apply smart_resize\n                        w, h = img.size\n                        new_h, new_w = smart_resize(h, w)\n                        img = img.resize((new_w, new_h), resample=Image.LANCZOS)\n                        \n                        part[\"image\"] = img\n        return messages\n\ndef collate_fn(batch):\n    full_result = processor.apply_chat_template(\n        batch,\n        tokenize=True,\n        return_dict=True,\n        truncation=True,            \n        max_length=Max_Context_Length,     \n        return_tensors=\"pt\",\n        padding=True,\n        pad_to_multiple_of=8\n    )\n\n    input_ids = full_result[\"input_ids\"]\n    labels = torch.full_like(input_ids, -100)\n\n    for batch_idx in range(input_ids.size(0)):\n        ids = input_ids[batch_idx].tolist()\n        L = len(ids)\n        pos = 0\n        while pos < L:\n            if ids[pos] == 77091:\n                ans_start = pos + 2\n                ans_end = ans_start\n                while ans_end < L and ids[ans_end] != 151645:\n                    ans_end += 1\n                if ans_end < L:\n                    labels[batch_idx, ans_start:ans_end + 2] = input_ids[batch_idx, ans_start:ans_end + 2]\n                    pos = ans_end\n            pos += 1\n\n    full_result[\"labels\"] = labels\n    return full_result\n\ntrain_dataset = QwenVisualDataset(train_df)\ntest_dataset = QwenVisualDataset(test_df)\n\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=1,\n    num_workers=1,\n    pin_memory=True,\n    persistent_workers=True,\n    shuffle=True, \n    collate_fn=collate_fn\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=1,\n    num_workers=1,\n    pin_memory=True,\n    persistent_workers=True,\n    shuffle=False, \n    collate_fn=collate_fn\n)\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:58.029814Z","iopub.execute_input":"2026-02-14T18:04:58.030090Z","iopub.status.idle":"2026-02-14T18:04:58.214135Z","shell.execute_reply.started":"2026-02-14T18:04:58.030054Z","shell.execute_reply":"2026-02-14T18:04:58.213512Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"], \n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\n\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:58.214987Z","iopub.execute_input":"2026-02-14T18:04:58.215788Z","iopub.status.idle":"2026-02-14T18:04:58.407265Z","shell.execute_reply.started":"2026-02-14T18:04:58.215751Z","shell.execute_reply":"2026-02-14T18:04:58.406594Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,605,632 || all params: 2,129,137,664 || trainable%: 0.0754\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from torch.optim import AdamW\nfrom tqdm import tqdm\nimport torch\nfrom torch.cuda.amp import GradScaler\n\ntrain_losses, test_losses = [], []\noptimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6)\nscaler = GradScaler()\naccumulation_steps = 4 \ntest_iter = iter(test_loader)\n\nmodel.train()\nfor epoch in range(1):\n    pbar = tqdm(enumerate(train_loader), total=20, desc=f\"Epoch {epoch}\")\n    for step, batch in pbar:\n        if step >= 20: \n            break\n            \n        batch = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n        if \"pixel_values\" in batch:\n            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(torch.float16)\n\n        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n            outputs = model(**batch)\n            raw_loss = outputs.loss\n            loss = raw_loss / accumulation_steps\n\n        if not torch.isfinite(raw_loss):\n            print(f\"NaN detected at step {step}. Skipping...\")\n            optimizer.zero_grad(set_to_none=True)\n            continue \n\n        scaler.scale(loss).backward()\n        train_losses.append(raw_loss.item())\n\n        model.eval()\n        try:\n            test_batch = next(test_iter)\n        except StopIteration:\n            test_iter = iter(test_loader)\n            test_batch = next(test_iter)\n        \n        test_batch = {k: v.to(\"cuda\") if isinstance(v, torch.Tensor) else v for k, v in test_batch.items()}\n        if \"pixel_values\" in test_batch: \n            test_batch[\"pixel_values\"] = test_batch[\"pixel_values\"].to(torch.float16)\n\n        with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n            test_outputs = model(**test_batch)\n            test_losses.append(test_outputs.loss.item())\n        model.train()\n\n        if (step + 1) % accumulation_steps == 0:\n            scaler.unscale_(optimizer)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n            \n        pbar.set_postfix({\"tr\": raw_loss.item(), \"te\": test_losses[-1]})\n\nmodel.save_pretrained(\"./qwen3_vl_adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-14T18:04:58.409471Z","iopub.execute_input":"2026-02-14T18:04:58.409714Z","iopub.status.idle":"2026-02-14T18:05:25.292278Z","shell.execute_reply.started":"2026-02-14T18:04:58.409690Z","shell.execute_reply":"2026-02-14T18:05:25.291283Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_1159/2481093753.py:8: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nEpoch 0:   5%|▌         | 1/20 [00:01<00:28,  1.51s/it]","output_type":"stream"},{"name":"stdout","text":"NaN detected at step 0. Skipping...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0:  15%|█▌        | 3/20 [00:03<00:20,  1.18s/it, tr=0.958, te=nan]","output_type":"stream"},{"name":"stdout","text":"NaN detected at step 2. Skipping...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0:  50%|█████     | 10/20 [00:13<00:11,  1.18s/it, tr=0.578, te=0.55]","output_type":"stream"},{"name":"stdout","text":"NaN detected at step 9. Skipping...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0:  55%|█████▌    | 11/20 [00:13<00:09,  1.05s/it, tr=0.578, te=0.55]","output_type":"stream"},{"name":"stdout","text":"NaN detected at step 10. Skipping...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0:  70%|███████   | 14/20 [00:18<00:07,  1.22s/it, tr=0.777, te=0.767]","output_type":"stream"},{"name":"stdout","text":"NaN detected at step 13. Skipping...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0:  75%|███████▌  | 15/20 [00:18<00:05,  1.08s/it, tr=0.777, te=0.767]","output_type":"stream"},{"name":"stdout","text":"NaN detected at step 14. Skipping...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0:  90%|█████████ | 18/20 [00:23<00:02,  1.24s/it, tr=0.578, te=0.506]","output_type":"stream"},{"name":"stdout","text":"NaN detected at step 17. Skipping...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 0: 100%|██████████| 20/20 [00:26<00:00,  1.32s/it, tr=0.616, te=0.784]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}